<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />






















<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=6.0.0" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.0.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.0.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.0.0" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Welcome！网站主要记录地震学学习过程，相关软件的使用方式，学习中发现的有趣的内容。 点击 Tags 选择某一特定软件的相关内容点击 Categories 选择某一特定学科的相关内容点击 Archives 按时间顺序查看内容     在茫茫的大千世界里，每一个人都应该保有一个自己的小千世界。  —— 林清玄《人生最美是清欢》">
<meta property="og:type" content="website">
<meta property="og:title" content="home">
<meta property="og:url" content="https://zhiyuanshi1901.github.io/index.html">
<meta property="og:site_name" content="Seismology Learning">
<meta property="og:description" content="Welcome！网站主要记录地震学学习过程，相关软件的使用方式，学习中发现的有趣的内容。 点击 Tags 选择某一特定软件的相关内容点击 Categories 选择某一特定学科的相关内容点击 Archives 按时间顺序查看内容     在茫茫的大千世界里，每一个人都应该保有一个自己的小千世界。  —— 林清玄《人生最美是清欢》">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-04-25T08:14:45.000Z">
<meta property="article:modified_time" content="2024-04-25T10:57:14.589Z">
<meta property="article:author" content="Shi Zhiyuan">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '6.0.0',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":true,"scrollpercent":true,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://ZhiyuanShi1901.github.io/"/>





  <title>Seismology Learning</title>
  








<meta name="generator" content="Hexo 7.2.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"> <div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Seismology Learning</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Seismology Learning Note</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-sitemap">
          <a href="/sitemap.xml" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-sitemap"></i> <br />
            
            Sitemap
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://ZhiyuanShi1901.github.io/2022/11/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/log.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Seismology Learning">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2022/11/21/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/" itemprop="url">深度学习初探</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2022-11-21T20:08:19+08:00">2022-11-21</time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>来源于：</p>
<p><a target="_blank" rel="noopener" href="http://introtodeeplearning.com/">MIT-Introduction to Deep Learning</a></p>
<p>代码为MIT课程练习LAB，可以使用Colab进行学习。链接：<br><a target="_blank" rel="noopener" href="https://github.com/aamini/introtodeeplearning">MIT-Codes Github resposibility</a></p>
<p>Information：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Copyright 2022 MIT 6.S191 Introduction to Deep Learning. All Rights Reserved.</span><br><span class="line"># </span><br><span class="line"># Licensed under the MIT License. You may not use this file except in compliance</span><br><span class="line"># with the License. Use and/or modification of this code outside of 6.S191 must</span><br><span class="line"># reference:</span><br><span class="line">#</span><br><span class="line"># © MIT 6.S191: Introduction to Deep Learning</span><br><span class="line"># http://introtodeeplearning.com</span><br></pre></td></tr></table></figure>

<h1 id="Lab-1-Intro-to-TensorFlow-and-Music-Generation-with-RNNs"><a href="#Lab-1-Intro-to-TensorFlow-and-Music-Generation-with-RNNs" class="headerlink" title="Lab 1: Intro to TensorFlow and Music Generation with RNNs"></a>Lab 1: Intro to TensorFlow and Music Generation with RNNs</h1><h2 id="Part-1-Intro-to-TensorFlow"><a href="#Part-1-Intro-to-TensorFlow" class="headerlink" title="Part 1:Intro to TensorFlow"></a>Part 1:Intro to TensorFlow</h2><h3 id="创建tf值"><a href="#创建tf值" class="headerlink" title="创建tf值"></a>创建tf值</h3><ul>
<li><p><code> x=tf.constant(&#39;String&#39;, tf.string)</code>:<br>创建一个常量，这个常量可以是多维列表。<br><code>tf.string</code>表示为字符串,<code>tf.float64</code>表示浮点型。</p>
</li>
<li><p><code>x=tf.Variable(value)</code>:创建一个变量</p>
</li>
<li><p><code>tf.zeros([a, b, c])</code>:创建一个全是0的矩阵,列表中表示size。</p>
</li>
</ul>
<h3 id="查看tf变量属性"><a href="#查看tf变量属性" class="headerlink" title="查看tf变量属性"></a>查看tf变量属性</h3><ul>
<li><code>tf.rank(value).numpy()</code>:查看value的维数</li>
<li><code>tf.shape(value).numpy()</code>:查看value的大小</li>
</ul>
<h3 id="tf计算函数"><a href="#tf计算函数" class="headerlink" title="tf计算函数"></a>tf计算函数</h3><ul>
<li><code>tf.add(a, b)</code>: a+b</li>
<li><code>tf.subtract(a, b)</code>: a-b</li>
<li><code>tf.multiply(a, b)</code>: a*b</li>
</ul>
<h3 id="创建一个简单的神经网络"><a href="#创建一个简单的神经网络" class="headerlink" title="创建一个简单的神经网络"></a>创建一个简单的神经网络</h3><p>使用的输出公式: <code>y=sigmoid(xW+b)</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">### Defining a network Layer ###</span><br><span class="line"></span><br><span class="line"># n_output_nodes: number of output nodes 输出节点个数</span><br><span class="line"># input_shape: shape of the input 输入大小</span><br><span class="line"># x: input to the layer 层的输入</span><br><span class="line"></span><br><span class="line">class OurDenseLayer(tf.keras.layers.Layer):  # 继承了Layer</span><br><span class="line">  def __init__(self, n_output_nodes):</span><br><span class="line">    super(OurDenseLayer, self).__init__()</span><br><span class="line">    self.n_output_nodes = n_output_nodes</span><br><span class="line"></span><br><span class="line">  def build(self, input_shape):</span><br><span class="line">    d = int(input_shape[-1])</span><br><span class="line">    # Define and initialize parameters: a weight matrix W and bias b  定义并初始化参数：权重W 误差b</span><br><span class="line">    # Note that parameter initialization is random! 参数的初始值是随机的！！！</span><br><span class="line">    self.W = self.add_weight(&quot;weight&quot;, shape=[d, self.n_output_nodes]) # note the dimensionality d行‘节点个数&#x27;列</span><br><span class="line">    self.b = self.add_weight(&quot;bias&quot;, shape=[1, self.n_output_nodes]) # note the dimensionality  1行 ‘节点个数&#x27;列</span><br><span class="line"></span><br><span class="line">  def call(self, x):</span><br><span class="line">    &#x27;&#x27;&#x27;TODO: define the operation for z (hint: use tf.matmul) 定义z &#x27;&#x27;&#x27;</span><br><span class="line">    z = tf.add(tf.matmul(x, self.W), self.b) # TODO</span><br><span class="line"></span><br><span class="line">    &#x27;&#x27;&#x27;TODO: define the operation for out (hint: use tf.sigmoid) 定义输出  &#x27;&#x27;&#x27;</span><br><span class="line">    y = tf.sigmoid(z) # TODO</span><br><span class="line">    return y</span><br><span class="line"></span><br><span class="line"># Since layer parameters are initialized randomly, we will set a random seed for reproducibility 层的参数初始化值是随机的，设置一个随机种子</span><br><span class="line">tf.random.set_seed(1)</span><br><span class="line">layer = OurDenseLayer(3)</span><br><span class="line">layer.build((1,2))</span><br><span class="line">x_input = tf.constant([[1,2.]], shape=(1,2))</span><br><span class="line">y = layer.call(x_input)</span><br><span class="line"></span><br><span class="line"># test the output!</span><br><span class="line">print(y.numpy())</span><br></pre></td></tr></table></figure>

<h3 id="使用时序-Sequential-API的神经网络"><a href="#使用时序-Sequential-API的神经网络" class="headerlink" title="使用时序(Sequential)API的神经网络"></a>使用时序(Sequential)API的神经网络</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras import Sequential</span><br><span class="line">from tensorflow.keras.layers import Dense</span><br><span class="line"></span><br><span class="line">n_output_nodes = 3</span><br><span class="line"></span><br><span class="line"># First define the model </span><br><span class="line">model = Sequential()</span><br><span class="line"></span><br><span class="line">dense_layer = Dense(units=n_output_nodes) # TODO  </span><br><span class="line"># 答案：dense_layer = Dense(n_output_nodes, activation=&#x27;sigmoid&#x27;) # TODO</span><br><span class="line"># Add the dense layer to the model</span><br><span class="line">model.add(dense_layer)</span><br></pre></td></tr></table></figure>
<p>测试输出：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x_input = tf.constant([[1,2.]], shape=(1,2))</span><br><span class="line">model_output = model(x_input) # TODO   将x输入model模型中运行。</span><br><span class="line"># 答案 model_output = model(x_input).numpy()</span><br><span class="line">print(model_output) </span><br></pre></td></tr></table></figure>

<p>使用Model类创建继承的子类SubclassModel：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">from tensorflow.keras import Model</span><br><span class="line">from tensorflow.keras.layers import Dense</span><br><span class="line">class SubclassModel(tf.keras.Model):</span><br><span class="line">    def __init__(self, n_output_nodes):</span><br><span class="line">        super(SubclassModel, self).__init__()</span><br><span class="line">        &#x27;&#x27;&#x27;TODO: Our model consists of a single Dense layer. Define this layer.&#x27;&#x27;&#x27; </span><br><span class="line">        self.dense_layer = Dense(n_output_nodes, activation=&#x27;sigmoid&#x27;)   &#x27;&#x27;&#x27;TODO: Dense Layer&#x27;&#x27;&#x27;</span><br><span class="line">        # 答案 self.dense_layer = Dense(n_output_nodes, activation=&#x27;sigmoid&#x27;)</span><br><span class="line">        # 此处使用sigmoid函数，若使用relu，输出为0；使用tanh，输出的0、2位置为负数。</span><br><span class="line">    def call(self, inputs):</span><br><span class="line">        return self.dense_layer(inputs)</span><br><span class="line"># 测试：   </span><br><span class="line">n_output_nodes = 3</span><br><span class="line">model = SubclassModel(n_output_nodes)</span><br><span class="line">x_input = tf.constant([[1,2.]], shape=(1,2))</span><br><span class="line">print(model.call(x_input))</span><br></pre></td></tr></table></figure>

<p>在<code>call</code>函数中增加：<code>isidentity=False</code>，<br>isidentity，“有时我们希望网络简单地输出原来的值”。<br>True时返回输入值，否则正常执行。</p>
<h3 id="梯度计算"><a href="#梯度计算" class="headerlink" title="梯度计算"></a>梯度计算</h3><p>通过<code>GradientTape()</code>方法实现梯度计算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable(3.0)</span><br><span class="line">with tf.GradientTape() as tape:</span><br><span class="line">    y=x*x</span><br><span class="line">dy_dx=tape.gradient(y, x)</span><br></pre></td></tr></table></figure>

<p>SGD梯度法优化方法：<br>计算损失L&#x3D;(x−x_f)^2</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">x = tf.Variable([tf.random.normal([1])]) # x随机取值</span><br><span class="line">learning_rate = 1e-2 # SGD的学习率</span><br><span class="line">history = []</span><br><span class="line">x_f = 4     # 定义目标值</span><br><span class="line"></span><br><span class="line">for i in range(500):</span><br><span class="line">    with tf.GradientTape() as tape:</span><br><span class="line">        loss = (x - x_f)**2  </span><br><span class="line">    grad = tape.gradient(loss, x) # 计算梯度值</span><br><span class="line">    new_x = x - learning_rate*grad # sgd update</span><br><span class="line">    x.assign(new_x) # 更新x值，即用new_x值替换x值。</span><br><span class="line">    history.append(x.numpy()[0]) # 加入索引是因为x为列表，但是x列表只有一个值。</span><br><span class="line"></span><br><span class="line"># Plot the evolution of x as we optimize towards x_f!</span><br><span class="line">plt.plot(history)</span><br><span class="line">plt.plot([0, 500],[x_f,x_f])</span><br><span class="line">plt.legend((&#x27;Predicted&#x27;, &#x27;True&#x27;))</span><br><span class="line">plt.xlabel(&#x27;Iteration&#x27;)</span><br><span class="line">plt.ylabel(&#x27;x value&#x27;)</span><br></pre></td></tr></table></figure>

<h1 id="Lab-2-Computer-Vision"><a href="#Lab-2-Computer-Vision" class="headerlink" title="Lab 2: Computer Vision"></a>Lab 2: Computer Vision</h1><h2 id="Part-1-MNIST-Digit-Classification-手写数字识别"><a href="#Part-1-MNIST-Digit-Classification-手写数字识别" class="headerlink" title="Part 1: MNIST Digit Classification 手写数字识别"></a>Part 1: MNIST Digit Classification 手写数字识别</h2><h3 id="初始模型创建"><a href="#初始模型创建" class="headerlink" title="初始模型创建"></a>初始模型创建</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">def build_fc_model():</span><br><span class="line">  fc_model = tf.keras.Sequential([</span><br><span class="line">      # First define a Flatten layer定义一个展平层</span><br><span class="line">      tf.keras.layers.Flatten(),</span><br><span class="line"></span><br><span class="line">      # &#x27;&#x27;&#x27;TODO: Define the activation function for the first fully connected (Dense) layer.&#x27;&#x27;&#x27;第一层全连接层激活函数</span><br><span class="line">      tf.keras.layers.Dense(128, activation=tf.nn.relu),</span><br><span class="line"></span><br><span class="line">      # &#x27;&#x27;&#x27;TODO: Define the second Dense layer to output the classification probabilities&#x27;&#x27;&#x27;定义第二层全连接层，输出分类概率</span><br><span class="line">      tf.keras.layers.Dense(10, activation=tf.nn.softmax) </span><br><span class="line">  ])</span><br><span class="line">  return fc_model</span><br><span class="line">model = build_fc_model()</span><br></pre></td></tr></table></figure>
<p>第一层全连接层使用<code>relu</code>激活函数，第二层全连接层用于输出预测的概率值，使用<code>softmax</code>激活函数.</p>
<p>这里使用<code>tf.nn.relu</code>形式，如果用<code>&#39;relu&#39;</code>会报错。</p>
<h3 id="模型编译"><a href="#模型编译" class="headerlink" title="模型编译"></a>模型编译</h3><p>在训练模型之前执行模型编译，包括设置优化器、损失函数、训练测试的指标：这里使用准确率。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=1e-1), </span><br><span class="line">              loss=&#x27;sparse_categorical_crossentropy&#x27;,</span><br><span class="line">              metrics=[&#x27;accuracy&#x27;])</span><br></pre></td></tr></table></figure>

<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>训练过程中显示损失和准确率。</p>
<ul>
<li>epochs：训练次数<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = 64</span><br><span class="line">EPOCHS = 5</span><br><span class="line">model.fit(train_images, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="使用测试训练集测试模型准确性"><a href="#使用测试训练集测试模型准确性" class="headerlink" title="使用测试训练集测试模型准确性"></a>使用测试训练集测试模型准确性</h3><p><code>evaluate</code>方法用来测试准确性。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc = model.evaluate(x=test_images, y=test_labels)</span><br><span class="line"></span><br><span class="line">print(&#x27;Test accuracy:&#x27;, test_acc)</span><br></pre></td></tr></table></figure>

<h3 id="创建CNN网络模型，并进行编译、训练和测试。"><a href="#创建CNN网络模型，并进行编译、训练和测试。" class="headerlink" title="创建CNN网络模型，并进行编译、训练和测试。"></a>创建CNN网络模型，并进行编译、训练和测试。</h3><p>创建模型：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">def build_cnn_model():</span><br><span class="line">    cnn_model = tf.keras.Sequential([</span><br><span class="line">        # 定义第一个卷积层</span><br><span class="line">        tf.keras.layers.Conv2D(filters=24, kernel_size=(3, 3), activation=&#x27;relu&#x27;), </span><br><span class="line">        # 定义第一个最大池化层</span><br><span class="line">        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),</span><br><span class="line">        # 定义第二个卷积层</span><br><span class="line">        tf.keras.layers.Conv2D(filters=36, kernel_size=(3, 3), activation=&#x27;relu&#x27;),</span><br><span class="line">        # 定义第二个最大池化层</span><br><span class="line">        tf.keras.layers.MaxPool2D(pool_size=(2, 2)),</span><br><span class="line">        tf.keras.layers.Flatten(),</span><br><span class="line">        tf.keras.layers.Dense(128, activation=tf.nn.relu),</span><br><span class="line">        # 输出分类概率，这一层输出为概率，因此输出个数为10，表示10个数字。</span><br><span class="line">        tf.keras.layers.Dense(10, activation=tf.nn.softmax),</span><br><span class="line">    ])</span><br><span class="line">    return cnn_model</span><br><span class="line">cnn_model = build_cnn_model()    # 输入一些数据进行模型初始化</span><br><span class="line">cnn_model.predict(train_images[[0]])    # 输出模型的一些摘要信息</span><br><span class="line">print(cnn_model.summary())</span><br></pre></td></tr></table></figure>
<p>编译:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=&#x27;sparse_categorical_crossentropy&#x27;, metrics=[&#x27;accuracy&#x27;]) </span><br></pre></td></tr></table></figure>
<p>训练：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cnn_model.fit(x=train_images, y=train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS)</span><br></pre></td></tr></table></figure>
<p>测试：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss, test_acc = cnn_model.evaluate(train_images, train_labels)</span><br><span class="line">print(&#x27;Test accuracy:&#x27;, test_acc)</span><br></pre></td></tr></table></figure>
<p>至此模型已经训练完毕，此时输入图片进行预测：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">predictions = cnn_model.predict(test_images)</span><br><span class="line">predictions[0] # 输出的概率列表</span><br><span class="line"></span><br><span class="line">prediction = np.argmax(predictions[0]) # 此为预测值</span><br><span class="line">print(prediction)</span><br><span class="line"></span><br><span class="line">print(&quot;Label of this digit is:&quot;, test_labels[0])</span><br><span class="line">plt.imshow(test_images[0,:,:,0], cmap=plt.cm.binary)    # 此为输出对应预测值的图像</span><br></pre></td></tr></table></figure>
<p>输出为：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">array([1.1896365e-08, 2.9328827e-08, 1.0020367e-07, 1.9016676e-07,</span><br><span class="line">       8.6228073e-11, 5.7627930e-11, 1.5786619e-13, 9.9999928e-01,</span><br><span class="line">       3.2206058e-09, 3.1025860e-07], dtype=float32)</span><br><span class="line">7</span><br></pre></td></tr></table></figure>
<p><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/L2P1_1.png"></p>
<h3 id="使用随机梯度下降训练CNN网络模型"><a href="#使用随机梯度下降训练CNN网络模型" class="headerlink" title="使用随机梯度下降训练CNN网络模型"></a>使用随机梯度下降训练CNN网络模型</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"># 重建CNN网络</span><br><span class="line">cnn_model = build_cnn_model()</span><br><span class="line"></span><br><span class="line">batch_size = 12</span><br><span class="line">loss_history = mdl.util.LossHistory(smoothing_factor=0.95) # 记录训练过程的损失</span><br><span class="line">plotter = mdl.util.PeriodicPlotter(sec=2, xlabel=&#x27;Iterations&#x27;, ylabel=&#x27;Loss&#x27;, scale=&#x27;semilogy&#x27;)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=1e-2) # 定义优化器</span><br><span class="line"></span><br><span class="line">if hasattr(tqdm, &#x27;_instances&#x27;): tqdm._instances.clear() # 如果存在，清除掉</span><br><span class="line"></span><br><span class="line">for idx in tqdm(range(0, train_images.shape[0], batch_size)):</span><br><span class="line">  # 首先抓取数据并将输入图像转化为张量。</span><br><span class="line">  (images, labels) = (train_images[idx:idx+batch_size], train_labels[idx:idx+batch_size])</span><br><span class="line">  images = tf.convert_to_tensor(images, dtype=tf.float32)</span><br><span class="line">  # 使用GradientTape方法记录梯度操作</span><br><span class="line">  with tf.GradientTape() as tape:</span><br><span class="line">    # 输入图片获得预测值</span><br><span class="line">    logits = cnn_model(images)</span><br><span class="line"></span><br><span class="line">    #&#x27;&#x27;&#x27;TODO: compute the categorical cross entropy loss</span><br><span class="line">    # 计算分类交叉熵损失(categorical cross entropy loss)</span><br><span class="line">    loss_value = tf.keras.backend.sparse_categorical_crossentropy(labels, logits) # 目标， 输出</span><br><span class="line">  loss_history.append(loss_value.numpy().mean()) # 将损失记录到记录着中。</span><br><span class="line">  plotter.plot(loss_history.get())</span><br><span class="line">  # 反向传播</span><br><span class="line">  # 使用trainable_variables获得预测值</span><br><span class="line">  # 计算参数梯度，d右/d左，利用apply_gradients计算，将梯度应用到变量中。</span><br><span class="line">  # zip()将变量重新打包，输入其中。</span><br><span class="line">  grads = tape.gradient(loss_value, cnn_model.trainable_variables)</span><br><span class="line">  optimizer.apply_gradients(zip(grads, cnn_model.trainable_variables))</span><br></pre></td></tr></table></figure>

<h2 id="Part-2-Debiasing-Facial-Detection-Systems-去偏面部检测系统"><a href="#Part-2-Debiasing-Facial-Detection-Systems-去偏面部检测系统" class="headerlink" title="Part 2: Debiasing Facial Detection Systems 去偏面部检测系统"></a>Part 2: Debiasing Facial Detection Systems 去偏面部检测系统</h2><h3 id="依赖项："><a href="#依赖项：" class="headerlink" title="依赖项："></a>依赖项：</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line">import IPython</span><br><span class="line">import functools</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">from tqdm import tqdm</span><br><span class="line">import mitdeeplearning as mdl</span><br></pre></td></tr></table></figure>

<h3 id="获取数据集"><a href="#获取数据集" class="headerlink" title="获取数据集"></a>获取数据集</h3><p>获取数据的网站：</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">Celeba</a>: 脸部图像数据集，此处为正向数据。</li>
<li><a target="_blank" rel="noopener" href="https://image-net.org/">ImageNet</a>:不同类别的图像集</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Fitzpatrick_scale">Fitzpatrick Scale</a>:肤色分类系统</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 从Celeba和ImageNet获取数据</span><br><span class="line">path_to_training_data = tf.keras.utils.get_file(&#x27;train_face.h5&#x27;, &#x27;https://www.dropbox.com/s/hlz8atheyozp1yx/train_face.h5?dl=1&#x27;)</span><br><span class="line"># 使用下载的数据集实例化训练数据集加载器</span><br><span class="line">loader = mdl.lab2.TrainingDatasetLoader(path_to_training_data)</span><br></pre></td></tr></table></figure>

<h3 id="定义并训练CNN网络模型"><a href="#定义并训练CNN网络模型" class="headerlink" title="定义并训练CNN网络模型"></a>定义并训练CNN网络模型</h3><p>定义四层卷积层，然后输出</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">n_filters = 12 # 卷积滤波器的base number </span><br><span class="line"># 定义标准CNN模型函数</span><br><span class="line">def make_standard_classifier(n_outputs=1):</span><br><span class="line">  Conv2D = functools.partial(tf.keras.layers.Conv2D, padding=&#x27;same&#x27;, activation=&#x27;relu&#x27;)  # 函数Conv2D的相应默认值修改为same和relu</span><br><span class="line">  BatchNormalization = tf.keras.layers.BatchNormalization</span><br><span class="line">  Flatten = tf.keras.layers.Flatten</span><br><span class="line">  Dense = functools.partial(tf.keras.layers.Dense, activation=&#x27;relu&#x27;)</span><br><span class="line"></span><br><span class="line">  model = tf.keras.Sequential([</span><br><span class="line">    Conv2D(filters=1*n_filters, kernel_size=5,  strides=2),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line">    </span><br><span class="line">    Conv2D(filters=2*n_filters, kernel_size=5,  strides=2),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2D(filters=4*n_filters, kernel_size=3,  strides=2),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Conv2D(filters=6*n_filters, kernel_size=3,  strides=2),</span><br><span class="line">    BatchNormalization(),</span><br><span class="line"></span><br><span class="line">    Flatten(),</span><br><span class="line">    Dense(512),</span><br><span class="line">    Dense(n_outputs, activation=None),</span><br><span class="line">  ])</span><br><span class="line">  return model</span><br><span class="line"></span><br><span class="line">standard_classifier = make_standard_classifier()</span><br></pre></td></tr></table></figure>
<p>对模型进行训练:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 训练使用的参数集</span><br><span class="line">batch_size = 32</span><br><span class="line">num_epochs = 2  # keep small to run faster  越小运算越快</span><br><span class="line">learning_rate = 5e-4</span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate) # define our optimizer</span><br><span class="line">loss_history = mdl.util.LossHistory(smoothing_factor=0.99) # to record loss evolution</span><br><span class="line">plotter = mdl.util.PeriodicPlotter(sec=2, scale=&#x27;semilogy&#x27;)</span><br><span class="line">if hasattr(tqdm, &#x27;_instances&#x27;): tqdm._instances.clear() # clear if it exists</span><br><span class="line"></span><br><span class="line">@tf.function</span><br><span class="line">def standard_train_step(x, y):</span><br><span class="line">  with tf.GradientTape() as tape:</span><br><span class="line">    # 将图像输入模型</span><br><span class="line">    logits = standard_classifier(x) </span><br><span class="line">    # 计算损失</span><br><span class="line">    loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits) # 目标值 预测值</span><br><span class="line"></span><br><span class="line">  # 反向传播，进行优化</span><br><span class="line">  grads = tape.gradient(loss, standard_classifier.trainable_variables)</span><br><span class="line">  optimizer.apply_gradients(zip(grads, standard_classifier.trainable_variables))</span><br><span class="line">  return loss</span><br><span class="line"></span><br><span class="line"># 进行训练循环</span><br><span class="line">for epoch in range(num_epochs):</span><br><span class="line">  for idx in tqdm(range(loader.get_train_size()//batch_size)):</span><br><span class="line">    # 抓取一批训练数据通过网络传播</span><br><span class="line">    x, y = loader.get_batch(batch_size)</span><br><span class="line">    loss = standard_train_step(x, y)</span><br><span class="line">    # 记录损失并将损失的演变绘制为训练的函数</span><br><span class="line">    loss_history.append(loss.numpy().mean())</span><br><span class="line">    plotter.plot(loss_history.get())</span><br></pre></td></tr></table></figure>
<p>性能评估：</p>
<ul>
<li>此部分使用CelebA数据集进行评估。<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 标准CNN网络模型</span><br><span class="line"># 使用CelebA+Imagenet两个数据集的子集进行评估</span><br><span class="line">(batch_x, batch_y) = loader.get_batch(5000)</span><br><span class="line">y_pred_standard = tf.round(tf.nn.sigmoid(standard_classifier.predict(batch_x)))</span><br><span class="line">acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))</span><br><span class="line">print(&quot;Standard CNN accuracy on (potentially biased) training set: &#123;:.4f&#125;&quot;.format(acc_standard.numpy()))</span><br></pre></td></tr></table></figure></li>
<li>使用未知数据集进行评估</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">### 评估在测试数据上的CNN网络模型</span><br><span class="line">standard_classifier_logits = [standard_classifier(np.array(x, dtype=np.float32)) for x in test_faces]</span><br><span class="line">standard_classifier_probs = tf.squeeze(tf.sigmoid(standard_classifier_logits))  # 删除为1的内容</span><br><span class="line"></span><br><span class="line"># Plot the prediction accuracies per demographic 绘制预测准确度</span><br><span class="line">xx = range(len(keys))</span><br><span class="line">yy = standard_classifier_probs.numpy().mean(1)</span><br><span class="line">plt.bar(xx, yy)</span><br><span class="line">plt.xticks(xx, keys)</span><br><span class="line">plt.ylim(max(0,yy.min()-yy.ptp()/2.), yy.max()+yy.ptp()/2.)</span><br><span class="line">plt.title(&quot;Standard classifier predictions&quot;);</span><br></pre></td></tr></table></figure>

<h3 id="用于学习隐结构-latent-structure-的变分自编码器-VAE"><a href="#用于学习隐结构-latent-structure-的变分自编码器-VAE" class="headerlink" title="用于学习隐结构(latent structure)的变分自编码器(VAE)"></a>用于学习隐结构(latent structure)的变分自编码器(VAE)</h3><p>对于某些训练集中不存在或者存在较少的特征，例如深色皮肤，戴帽子的人等，<br>这些特征可以通过无监督的学习方式，使用VAE进行训练。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 定义VAE损失函数</span><br><span class="line">&#x27;&#x27;&#x27; Function to calculate VAE loss given:</span><br><span class="line">      输入 x, </span><br><span class="line">      重构输出 x_recon, </span><br><span class="line">      编码均值 mu, </span><br><span class="line">      编码标准偏差的log值 logsigma, </span><br><span class="line">      隐损失的权重参数 kl_weight</span><br><span class="line">&#x27;&#x27;&#x27;</span><br><span class="line">def vae_loss_function(x, x_recon, mu, logsigma, kl_weight=0.0005):</span><br><span class="line">  # 定义隐损失函数:Latent_Loss=0.5*sum(sigmaj+mu^2-1-log(sigmaj))</span><br><span class="line">  latent_loss = 1/2 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1 - logsigma, axis=1) #</span><br><span class="line"></span><br><span class="line">  # https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean</span><br><span class="line">  # 定义L1规范的reconstruction loss，即重构损失:reconstruction_Loss=||x-x_recon)||1 计算输入与重构输入的1范数。</span><br><span class="line">  reconstruction_loss = tf.reduce_mean(tf.abs(x - x_recon), axis=(1, 2, 3)) # 即使使用这种axis，可能是按照不同顺序计算维度的平均值，最后得到的是一个数值，即number而非张量</span><br><span class="line"></span><br><span class="line">  # 定义VAE损失:Lvae=c*LatentLoss+Reconstruction_loss</span><br><span class="line">  vae_loss = kl_weight * latent_loss + reconstruction_loss</span><br><span class="line">  return vae_loss</span><br></pre></td></tr></table></figure>

<p>VAEs重参数计算；</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 输入为：隐分布（潜在分布）均值、隐分布的log值</span><br><span class="line"># 输出为：z张量，采样潜在向量</span><br><span class="line">def sampling(z_mean, z_logsigma):</span><br><span class="line">  # By default, random.normal is &quot;standard&quot; (ie. mean=0 and std=1.0)</span><br><span class="line">  batch, latent_dim = z_mean.shape</span><br><span class="line">  epsilon = tf.random.normal(shape=(batch, latent_dim))</span><br><span class="line"></span><br><span class="line">  # 定义重参数化计算：z=mu+exp(0.5*log(sigma))*epsilon</span><br><span class="line">  z = z_mean + tf.math.exp(z_logsigma / 2) * epsilon  # b不知道这里tf.math.exp需不需要加math，因为前面的reduce_mean没有加math</span><br><span class="line">  return z</span><br></pre></td></tr></table></figure>

<h3 id="DB-VAE-去偏变分自编码器"><a href="#DB-VAE-去偏变分自编码器" class="headerlink" title="DB-VAE 去偏变分自编码器"></a>DB-VAE 去偏变分自编码器</h3><p>将具有更低出现频率的特征增加采样，而高频特征减少采样，达到平均采样的目的。<br>具体流程图如下：<br><img src="/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E6%8E%A2/L2P2_2.png" alt="2"></p>
<p>DB损失函数：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 输入：输入值，重构值，真正的标签值，预测的标签值，隐分布的均值，隐分布标准方差的log值</span><br><span class="line">def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):</span><br><span class="line"></span><br><span class="line">  # 使用VAE损失函数计算VAE损失</span><br><span class="line">  vae_loss = vae_loss_function(x, x_pred, mu, logsigma) # TODO</span><br><span class="line">  </span><br><span class="line">  # sigmoid_cross_entropy_with_logits计算分类损失</span><br><span class="line">  # https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits</span><br><span class="line">  classification_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=y , logits=y_logit)</span><br><span class="line"></span><br><span class="line">  # 使用训练数据标签创建变量：该变量反映是否是人脸数据的指标</span><br><span class="line">  face_indicator = tf.cast(tf.equal(y, 1), tf.float32)</span><br><span class="line"></span><br><span class="line">  # 定义总损失</span><br><span class="line">  total_loss = tf.reduce_mean(classification_loss + face_indicator * vae_loss)</span><br><span class="line"></span><br><span class="line">  return total_loss, classification_loss</span><br><span class="line">  # 输出：DBVAE的总损失，分类损失</span><br></pre></td></tr></table></figure>

<p>定义DBVAE的解码器部分：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># 解码器部分</span><br><span class="line">n_filters = 12 # 卷积滤波器的base number，和CNN一样</span><br><span class="line">latent_dim = 100 # 因变量的数量</span><br><span class="line"></span><br><span class="line">def make_face_decoder_network():</span><br><span class="line">  # 定义不同的层</span><br><span class="line">  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding=&#x27;same&#x27;, activation=&#x27;relu&#x27;)</span><br><span class="line">  BatchNormalization = tf.keras.layers.BatchNormalization</span><br><span class="line">  Flatten = tf.keras.layers.Flatten</span><br><span class="line">  Dense = functools.partial(tf.keras.layers.Dense, activation=&#x27;relu&#x27;)</span><br><span class="line">  Reshape = tf.keras.layers.Reshape</span><br><span class="line"></span><br><span class="line">  # 使用Sequential构建解码器</span><br><span class="line">  decoder = tf.keras.Sequential([</span><br><span class="line">    # Transform to pre-convolutional generation</span><br><span class="line">    Dense(units=4*4*6*n_filters),  # 4x4 feature maps (with 6N occurances)</span><br><span class="line">    Reshape(target_shape=(4, 4, 6*n_filters)),</span><br><span class="line"></span><br><span class="line">    # Upscaling convolutions (inverse of encoder)</span><br><span class="line">    Conv2DTranspose(filters=4*n_filters, kernel_size=3,  strides=2),</span><br><span class="line">    Conv2DTranspose(filters=2*n_filters, kernel_size=3,  strides=2),</span><br><span class="line">    Conv2DTranspose(filters=1*n_filters, kernel_size=5,  strides=2),</span><br><span class="line">    Conv2DTranspose(filters=3, kernel_size=5,  strides=2),</span><br><span class="line">  ])</span><br><span class="line">  return decoder</span><br></pre></td></tr></table></figure>

<p>定义和创建DB—VAE网络:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">class DB_VAE(tf.keras.Model):</span><br><span class="line">  def __init__(self, latent_dim):</span><br><span class="line">    super(DB_VAE, self).__init__()</span><br><span class="line">    self.latent_dim = latent_dim</span><br><span class="line"></span><br><span class="line">    # Define the number of outputs for the encoder. Recall that we have </span><br><span class="line">    # `latent_dim` latent variables, as well as a supervised output for the </span><br><span class="line">    # classification.</span><br><span class="line">    # 定义编码器的输出数量。</span><br><span class="line">    num_encoder_dims = 2*self.latent_dim + 1</span><br><span class="line"></span><br><span class="line">    self.encoder = make_standard_classifier(num_encoder_dims)</span><br><span class="line">    self.decoder = make_face_decoder_network()</span><br><span class="line"></span><br><span class="line">  # function to feed images into encoder, encode the latent space, and output</span><br><span class="line">  #   classification probability</span><br><span class="line">  # 定义编码器，输出预测值，mu和logsigma </span><br><span class="line">  def encode(self, x):</span><br><span class="line">    encoder_output = self.encoder(x)   # 编码器输出</span><br><span class="line">    # classification prediction 分类预测值</span><br><span class="line">    y_logit = tf.expand_dims(encoder_output[:, 0], -1)</span><br><span class="line">    # latent variable distribution parameters 因变量分布参数</span><br><span class="line">    z_mean = encoder_output[:, 1:self.latent_dim+1] </span><br><span class="line">    z_logsigma = encoder_output[:, self.latent_dim+1:]</span><br><span class="line">    return y_logit, z_mean, z_logsigma</span><br><span class="line"></span><br><span class="line">  # VAE reparameterization: given a mean and logsigma, sample latent variables</span><br><span class="line">  def reparameterize(self, z_mean, z_logsigma):</span><br><span class="line">    # VAE重参数构建</span><br><span class="line">    z = sampling(z_mean, z_logsigma)</span><br><span class="line">    return z</span><br><span class="line"></span><br><span class="line">  # 解码因空间，输出输入的重建</span><br><span class="line">  def decode(self, z):</span><br><span class="line">    reconstruction = self.decoder(z)  # why???</span><br><span class="line">    return reconstruction</span><br><span class="line"></span><br><span class="line">  # The call function will be used to pass inputs x through the core VAE</span><br><span class="line">  def call(self, x): </span><br><span class="line">    # Encode input to a prediction and latent space</span><br><span class="line">    y_logit, z_mean, z_logsigma = self.encode(x)</span><br><span class="line">    # 重参数化</span><br><span class="line">    z = self.reparameterize(z_mean, z_logsigma)</span><br><span class="line">    # 使用decode重构建</span><br><span class="line">    recon = self.decode(z)</span><br><span class="line">    return y_logit, z_mean, z_logsigma, recon</span><br><span class="line"></span><br><span class="line">  # 预测是否是人脸</span><br><span class="line">  def predict(self, x):</span><br><span class="line">    y_logit, z_mean, z_logsigma = self.encode(x)</span><br><span class="line">    return y_logit</span><br><span class="line"></span><br><span class="line">dbvae = DB_VAE(latent_dim)</span><br></pre></td></tr></table></figure>


<h3 id="实现DB-VAE"><a href="#实现DB-VAE" class="headerlink" title="实现DB-VAE"></a>实现DB-VAE</h3><p>定义一个辅助函数，输出隐变量均值</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def get_latent_mu(images, dbvae, batch_size=1024):</span><br><span class="line">  N = images.shape[0]</span><br><span class="line">  mu = np.zeros((N, latent_dim))</span><br><span class="line">  for start_ind in range(0, N, batch_size):</span><br><span class="line">    end_ind = min(start_ind+batch_size, N+1)</span><br><span class="line">    batch = (images[start_ind:end_ind]).astype(np.float32)/255.</span><br><span class="line">    _, batch_mu, _ = dbvae.encode(batch)</span><br><span class="line">    mu[start_ind:end_ind] = batch_mu</span><br><span class="line">  return mu</span><br></pre></td></tr></table></figure>

<p>重新定义重采样算法：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 根据图像在训练数据中的分布重新计算批次中图像的采样概率的函数</span><br><span class="line">def get_training_sample_probabilities(images, dbvae, bins=10, smoothing_fac=0.001): </span><br><span class="line">    print(&quot;Recomputing the sampling probabilities&quot;)</span><br><span class="line">    mu = get_latent_mu(images, dbvae) # 获得潜在变量均值</span><br><span class="line">    training_sample_p = np.zeros(mu.shape[0]) # 图像采样概率</span><br><span class="line"></span><br><span class="line">    # 考虑每个潜在变量的分布</span><br><span class="line">    for i in range(latent_dim):</span><br><span class="line"></span><br><span class="line">        latent_distribution = mu[:,i]</span><br><span class="line">        # generate a histogram of the latent distribution</span><br><span class="line">        # 潜在分布直方图</span><br><span class="line">        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)</span><br><span class="line"></span><br><span class="line">        # find which latent bin every data sample falls in </span><br><span class="line">        bin_edges[0] = -float(&#x27;inf&#x27;)</span><br><span class="line">        bin_edges[-1] = float(&#x27;inf&#x27;)</span><br><span class="line">        </span><br><span class="line">        # call the digitize function to find which bins in the latent distribution every data sample falls in to</span><br><span class="line">        # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.digitize.html</span><br><span class="line">        bin_idx = np.digitize(latent_distribution, bin_edges)O</span><br><span class="line"></span><br><span class="line">        # 平滑密度函数</span><br><span class="line">        hist_smoothed_density = hist_density + smoothing_fac</span><br><span class="line">        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)</span><br><span class="line"></span><br><span class="line">        p = 1.0/(hist_smoothed_density[bin_idx-1])# 反转密度函数</span><br><span class="line">        p = p/np.sum(p)     # 将概率归一化</span><br><span class="line">        training_sample_p = np.maximum(p, training_sample_p)  # 选择较大的p值作为采样概率</span><br><span class="line">    training_sample_p /= np.sum(training_sample_p) # 最终归一化</span><br><span class="line"></span><br><span class="line">    return training_sample_p</span><br></pre></td></tr></table></figure>

<p>训练DB-VAE网络：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Hyperparameters</span><br><span class="line">batch_size = 32</span><br><span class="line">learning_rate = 5e-4</span><br><span class="line">latent_dim = 100</span><br><span class="line"></span><br><span class="line"># DB-VAE训练次数增加，因为它更加复杂。</span><br><span class="line">num_epochs = 6  </span><br><span class="line"></span><br><span class="line"># 创建一个DB-VAE对象dbvae，以及一个Adam优化器</span><br><span class="line">dbvae = DB_VAE(100)</span><br><span class="line">optimizer = tf.keras.optimizers.Adam(learning_rate)</span><br><span class="line"></span><br><span class="line"># 使用tf.function使得得以绘制直方图</span><br><span class="line">@tf.function</span><br><span class="line">def debiasing_train_step(x, y):</span><br><span class="line"></span><br><span class="line">  with tf.GradientTape() as tape:</span><br><span class="line">  # 输入x进入dbvae</span><br><span class="line">    y_logit, z_mean, z_logsigma, x_recon = dbvae(x)</span><br><span class="line"></span><br><span class="line">  # 计算损失</span><br><span class="line">    loss, class_loss = debiasing_loss_function(x=x, x_pred=x_recon, y=y, y_logit=y_logit, mu=z_mean, logsigma=z_logsigma) # TODO</span><br><span class="line">  </span><br><span class="line">  # 使用GradientTape.gradient计算梯度</span><br><span class="line">  grads = tape.gradient(loss, dbvae.trainable_variables) </span><br><span class="line"></span><br><span class="line">  # apply gradients to variables 调用梯度到优化器</span><br><span class="line">  optimizer.apply_gradients(zip(grads, dbvae.trainable_variables))</span><br><span class="line">  return loss</span><br><span class="line"></span><br><span class="line"># 加载数据集</span><br><span class="line">all_faces = loader.get_all_train_faces()</span><br><span class="line"></span><br><span class="line">if hasattr(tqdm, &#x27;_instances&#x27;): tqdm._instances.clear() # 存在则清除</span><br><span class="line"></span><br><span class="line"># 训练循环</span><br><span class="line">for i in range(num_epochs):</span><br><span class="line">  IPython.display.clear_output(wait=True)</span><br><span class="line">  print(&quot;Starting epoch &#123;&#125;/&#123;&#125;&quot;.format(i+1, num_epochs))</span><br><span class="line"></span><br><span class="line">  # Recompute data sampling proabilities 重新计算数据采样概率</span><br><span class="line">  p_faces = get_training_sample_probabilities(images=all_faces ,dbvae=dbvae)</span><br><span class="line">  </span><br><span class="line">  # 获取一批训练数据并计算训练步长</span><br><span class="line">  for j in tqdm(range(loader.get_train_size() // batch_size)):</span><br><span class="line">    # load a batch of data</span><br><span class="line">    (x, y) = loader.get_batch(batch_size, p_pos=p_faces)</span><br><span class="line">    # loss optimization</span><br><span class="line">    loss = debiasing_train_step(x, y)</span><br><span class="line">    </span><br><span class="line">    # plot the progress every 200 steps</span><br><span class="line">    if j % 500 == 0: </span><br><span class="line">      mdl.util.plot_sample(x, y, dbvae)</span><br></pre></td></tr></table></figure>

<p>准确性评估：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">dbvae_logits = [dbvae.predict(np.array(x, dtype=np.float32)) for x in test_faces]</span><br><span class="line">dbvae_probs = tf.squeeze(tf.sigmoid(dbvae_logits))</span><br><span class="line"></span><br><span class="line">xx = np.arange(len(keys))</span><br><span class="line">plt.bar(xx, standard_classifier_probs.numpy().mean(1), width=0.2, label=&quot;Standard CNN&quot;)</span><br><span class="line">plt.bar(xx+0.2, dbvae_probs.numpy().mean(1), width=0.2, label=&quot;DB-VAE&quot;)</span><br><span class="line">plt.xticks(xx, keys); </span><br><span class="line">plt.title(&quot;Network predictions on test dataset&quot;)</span><br><span class="line">plt.ylabel(&quot;Probability&quot;); plt.legend(bbox_to_anchor=(1.04,1), loc=&quot;upper left&quot;);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/default-index/page/29/">&lt;</a><a class="page-number" href="/default-index/">1</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/29/">29</a><span class="page-number current">30</span><a class="page-number" href="/default-index/page/31/">31</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/61/">61</a><a class="extend next" rel="next" href="/default-index/page/31/">&gt;</a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/log.png"
                alt="" />
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/">
              
                  <span class="site-state-item-count">61</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">32</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/ZhiyuanShi1901" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:Shizhiyuan2001@gmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://twitter.com/ShiZhiyuan_Seis" target="_blank" title="Twitter">
                      
                        <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://youtube.com/@user-bu6jx5vo6b" target="_blank" title="YouTube">
                      
                        <i class="fa fa-fw fa-youtube"></i>YouTube</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://instagram.com/@dshi78066" target="_blank" title="Instagram">
                      
                        <i class="fa fa-fw fa-instagram"></i>Instagram</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/shi-zhi-yuan-84" target="_blank" title="Zhihu">
                      
                        <i class="fa fa-fw fa-zhihu"></i>Zhihu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      
        <div class="back-to-top">
          <i class="fa fa-arrow-up"></i>
          
            <span id="scrollpercent"><span>0</span>%</span>
          
        </div>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2022 &mdash; <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shi Zhiyuan</span>

  

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/theme-next/hexo-theme-next">NexT.Muse</a> v6.0.0</div>




        







        
      </div>
    </footer>

    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
























  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=6.0.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=6.0.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=6.0.0"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  
  <script type="text/javascript" src="/js/src/js.cookie.js?v=6.0.0"></script>
  <script type="text/javascript" src="/js/src/scroll-cookie.js?v=6.0.0"></script>


  

</body>
</html>
